{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><p style=\"text-align: center;\"> <u> Song Popularity Prediction </u>  </p> </h1>\n<h3><p style=\"text-align: center;\"> <i> Tabular The Data Is </i>  </p> </h3>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.gifer.com/9mhx.gif\" width=\"500\" />","metadata":{}},{"cell_type":"markdown","source":"#### *Yeah, you guessed it right, my favourite musician*","metadata":{}},{"cell_type":"code","source":"# Import Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom xgboost import XGBClassifier,XGBRegressor,plot_importance,XGBRFRegressor\n\nfrom scipy.stats import mode,boxcox,skew\n\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,PowerTransformer\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer,KNNImputer,IterativeImputer\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom category_encoders import target_encoder\nimport seaborn as sns\nimport gc\nimport sys,os\n\nfrom scipy.spatial import distance\ngc.enable()\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T11:46:03.408419Z","iopub.execute_input":"2022-01-29T11:46:03.408705Z","iopub.status.idle":"2022-01-29T11:46:03.416164Z","shell.execute_reply.started":"2022-01-29T11:46:03.408676Z","shell.execute_reply":"2022-01-29T11:46:03.415505Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### 1. Read Train, Test and Submission files","metadata":{}},{"cell_type":"code","source":"# Read Train, Test and Sample Submission Files\ndef read_data():\n    df_train = pd.read_csv(\"../input/song-popularity-prediction/train.csv\")\n    df_test = pd.read_csv(\"../input/song-popularity-prediction/test.csv\")\n    df_submission = pd.read_csv(\"../input/song-popularity-prediction/sample_submission.csv\")\n    return df_train,df_test,df_submission","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:56.031063Z","iopub.execute_input":"2022-01-29T11:40:56.031679Z","iopub.status.idle":"2022-01-29T11:40:56.037539Z","shell.execute_reply.started":"2022-01-29T11:40:56.031640Z","shell.execute_reply":"2022-01-29T11:40:56.035738Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read datasets\ndf_train,df_test,df_submission = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:56.209848Z","iopub.execute_input":"2022-01-29T11:40:56.210289Z","iopub.status.idle":"2022-01-29T11:40:56.477165Z","shell.execute_reply.started":"2022-01-29T11:40:56.210257Z","shell.execute_reply":"2022-01-29T11:40:56.476461Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Let's see how you look.. any missing values.. ANY?","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:56.594906Z","iopub.execute_input":"2022-01-29T11:40:56.595227Z","iopub.status.idle":"2022-01-29T11:40:56.608178Z","shell.execute_reply.started":"2022-01-29T11:40:56.595195Z","shell.execute_reply":"2022-01-29T11:40:56.607406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:56.802526Z","iopub.execute_input":"2022-01-29T11:40:56.802735Z","iopub.status.idle":"2022-01-29T11:40:56.823914Z","shell.execute_reply.started":"2022-01-29T11:40:56.802711Z","shell.execute_reply":"2022-01-29T11:40:56.823116Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:56.982177Z","iopub.execute_input":"2022-01-29T11:40:56.982733Z","iopub.status.idle":"2022-01-29T11:40:56.995423Z","shell.execute_reply.started":"2022-01-29T11:40:56.982701Z","shell.execute_reply":"2022-01-29T11:40:56.994422Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Insight**: Both Test and Train have same columns which have NULL values .. Nice","metadata":{}},{"cell_type":"code","source":"df_train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:57.345117Z","iopub.execute_input":"2022-01-29T11:40:57.345325Z","iopub.status.idle":"2022-01-29T11:40:57.407506Z","shell.execute_reply.started":"2022-01-29T11:40:57.345300Z","shell.execute_reply":"2022-01-29T11:40:57.406656Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_test.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:57.545119Z","iopub.execute_input":"2022-01-29T11:40:57.546341Z","iopub.status.idle":"2022-01-29T11:40:57.605069Z","shell.execute_reply.started":"2022-01-29T11:40:57.546296Z","shell.execute_reply":"2022-01-29T11:40:57.604385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### What does each variable mean?\n> - **Instrumentalness:** This value represents the amount of vocals in the song. The closer it is to 1.0, the more instrumental the song is.\n> - **Acousticness:** This value describes how acoustic a song is. A score of 1.0 means the song is most likely to be an acoustic one.\n> - **Liveness:** This value describes the probability that the song was recorded with a live audience. According to the official documentation “a value above 0.8 provides strong likelihood that the track is live”.\n> - **Speechiness:** “Speechiness detects the presence of spoken words in a track”. If the speechiness of a song is above 0.66, it is probably made of spoken words, a score between 0.33 and 0.66 is a song that may contain both music and words, and a score below 0.33 means the song does not have any speech.\n> - **Energy:** “(energy) represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy”.\n> - **Danceability:** “Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable”.\n> - **Valence:** “A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)”\n> - **Song Duration ms :** Duration of song in milliseconds\n> - **Audio Mode :** No specific description\n> - **Tempo :** Tempo (Italian for \"time\"; plural tempos, or tempi from the Italian plural) is the speed or pace of a given piece.For example, a tempo of 60 beats per minute signifies one beat per second, while a tempo of 120 beats per minute is twice as rapid, signifying one beat every 0.5 seconds\n> - **Time Signature :** The time signature indicates how many counts are in each measure and which type of note will receive one count. The top number is commonly 2, 3, 4, or 6.The bottom number is either 4 or 8. Simple time signatures divide music into groups of 2 and compound divide music into groups of 3.\n> - **Loudness:** Loudness measures the decibel level of a song. Decibels are relative to a reference value, so songs with lower loudness values are quieter relative to the reference value of 0.\n> - **Danceability:** Danceability quantifies how suitable a track is for dancing based on a combination of musical elements, like tempo, rhythm, and beat. Songs with higher danceability have stronger and more regular beats.Like acousticness, danceability is measured on a scale of 0 (low danceability) to 100 (high danceability).","metadata":{}},{"cell_type":"code","source":"cont_cols = [col for col in df_train.columns if col not in ['id','song_popularity','audio_mode','time_signature','key']]\ncat_cols = ['audio_mode','time_signature','key']","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:57.897978Z","iopub.execute_input":"2022-01-29T11:40:57.898211Z","iopub.status.idle":"2022-01-29T11:40:57.903632Z","shell.execute_reply.started":"2022-01-29T11:40:57.898184Z","shell.execute_reply":"2022-01-29T11:40:57.901406Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(f'% Distribution of Song Popularity:\\n{(df_train.song_popularity.value_counts())/len(df_train.song_popularity)*100}')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:58.073547Z","iopub.execute_input":"2022-01-29T11:40:58.073744Z","iopub.status.idle":"2022-01-29T11:40:58.079795Z","shell.execute_reply.started":"2022-01-29T11:40:58.073720Z","shell.execute_reply":"2022-01-29T11:40:58.079046Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train.key.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:58.279749Z","iopub.execute_input":"2022-01-29T11:40:58.280032Z","iopub.status.idle":"2022-01-29T11:40:58.288229Z","shell.execute_reply.started":"2022-01-29T11:40:58.280004Z","shell.execute_reply":"2022-01-29T11:40:58.287511Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_train.audio_mode.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:58.539690Z","iopub.execute_input":"2022-01-29T11:40:58.539891Z","iopub.status.idle":"2022-01-29T11:40:58.548970Z","shell.execute_reply.started":"2022-01-29T11:40:58.539868Z","shell.execute_reply":"2022-01-29T11:40:58.548143Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train.time_signature.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:58.630070Z","iopub.execute_input":"2022-01-29T11:40:58.630254Z","iopub.status.idle":"2022-01-29T11:40:58.637968Z","shell.execute_reply.started":"2022-01-29T11:40:58.630232Z","shell.execute_reply":"2022-01-29T11:40:58.637248Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 2. Plot the distribution of data for each column","metadata":{}},{"cell_type":"markdown","source":"![Alt text](https://media.makeameme.org/created/if-you-didnt-58c83d63de.jpg)","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    df_train.hist(col,grid=False,bins = 12)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:40:59.890961Z","iopub.execute_input":"2022-01-29T11:40:59.891865Z","iopub.status.idle":"2022-01-29T11:41:00.453005Z","shell.execute_reply.started":"2022-01-29T11:40:59.891823Z","shell.execute_reply":"2022-01-29T11:41:00.452314Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"> **I NEED SOME CHARTS.. NOW**","metadata":{}},{"cell_type":"markdown","source":"Hmm... Below plots will show the distribution of data against our target variables.. Please be nice","metadata":{}},{"cell_type":"code","source":"for col in cont_cols:\n    sns.displot(data=df_train, x=col,hue=\"song_popularity\",kind = \"kde\", fill=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:00.454414Z","iopub.execute_input":"2022-01-29T11:41:00.454694Z","iopub.status.idle":"2022-01-29T11:41:06.134814Z","shell.execute_reply.started":"2022-01-29T11:41:00.454658Z","shell.execute_reply":"2022-01-29T11:41:06.134018Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"> Let's check box plots for those.. outliers","metadata":{}},{"cell_type":"code","source":"for col in cont_cols:\n    plt.figure()\n    sns.boxplot(data = df_train,x='song_popularity',y = col)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:06.136629Z","iopub.execute_input":"2022-01-29T11:41:06.136885Z","iopub.status.idle":"2022-01-29T11:41:07.743392Z","shell.execute_reply.started":"2022-01-29T11:41:06.136850Z","shell.execute_reply":"2022-01-29T11:41:07.742743Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### I know my visualization skills need a serious upgrade\n\n#### *Working..... On it........*","metadata":{}},{"cell_type":"code","source":"# Check correlation between the columns\ncorr = df_train.drop(['id','song_popularity'],axis=1).corr()\ncm = sns.light_palette(\"green\", as_cmap=True)\ncm = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\ncorr.style.background_gradient(cmap=cm)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.744603Z","iopub.execute_input":"2022-01-29T11:41:07.744925Z","iopub.status.idle":"2022-01-29T11:41:07.861747Z","shell.execute_reply.started":"2022-01-29T11:41:07.744887Z","shell.execute_reply":"2022-01-29T11:41:07.861065Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"> **Let's Create a function below to Scale our data**","metadata":{}},{"cell_type":"code","source":"# Function to Scale and transform dataset\ndef data_scaler_fit(option,df):\n    if option == 1:\n        transformer = StandardScaler().fit(df)\n    if option == 2 :\n        transformer = RobustScaler().fit(df)\n    if option == 3 :\n        transformer = MinMaxScaler().fit(df)\n    if option == 4 :\n        transformer = PowerTransformer(method = 'yeo-johnson').fit(df)\n    return transformer","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.863494Z","iopub.execute_input":"2022-01-29T11:41:07.864188Z","iopub.status.idle":"2022-01-29T11:41:07.870535Z","shell.execute_reply.started":"2022-01-29T11:41:07.864148Z","shell.execute_reply":"2022-01-29T11:41:07.869748Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"> **A Function to remove outlier, the standard way**","metadata":{}},{"cell_type":"code","source":"# Function to Remove outliers\ndef remove_outliers(x,method):\n    if method == 'mean':\n        upper_limit = x.mean() + (3*x.std())\n        lower_limit = x.mean() - (3*x.std())\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    elif method == 'median':\n        upper_limit = x.median() + (1.5*x.quantile(0.75))\n        lower_limit = x.median() - (1.5*x.quantile(0.25))\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    else:\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.871670Z","iopub.execute_input":"2022-01-29T11:41:07.872352Z","iopub.status.idle":"2022-01-29T11:41:07.880365Z","shell.execute_reply.started":"2022-01-29T11:41:07.872309Z","shell.execute_reply":"2022-01-29T11:41:07.879634Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def imputations(impute_method,method=None):\n    if impute_method == 'knn':\n        imputer = KNNImputer(n_neighbors = 10,weights = 'distance')\n    if impute_method == 'iter':\n        imputer = IterativeImputer(max_iter=20)\n    if impute_method == 'simple':\n        imputer = SimpleImputer(strategy=method)\n    if impute_method == 'lgbm':\n        if not os.path.exists(\"kuma_utils/\"):\n            !git clone https://github.com/analokmaus/kuma_utils.git\n        sys.path.append(\"kuma_utils/\")\n        from kuma_utils.preprocessing.imputer import LGBMImputer\n        imputer = LGBMImputer(n_iter=300, verbose=False)\n    return imputer","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.881788Z","iopub.execute_input":"2022-01-29T11:41:07.882260Z","iopub.status.idle":"2022-01-29T11:41:07.892623Z","shell.execute_reply.started":"2022-01-29T11:41:07.882225Z","shell.execute_reply":"2022-01-29T11:41:07.891872Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### 3. Let's Transform those features, one at a time","metadata":{}},{"cell_type":"markdown","source":"> 1. Column **Key**,**audio_mode**,**time_signature**  seems to be categorical, hence replacing null values using KNN\n> 2. Convert **time_signature** from milliseconds to minutes\n> 3. Fill Rest of the columns which has null values using mean of the columns (Might replace with something better)\n> 4. Use Boxcox transform to reduce skewness in continuous columns\n> 5. Finally using StandarScaler on all continuous columns : *It transforms the data in such a manner that it has mean as 0 and standard deviation as 1*","metadata":{}},{"cell_type":"code","source":"def feature_transform(df,option,method):\n    ids = df.id.values.tolist()\n    \n    # Replace missing values for continuous columns\n    impute = imputations(impute_method)\n    \n    df_temp = pd.DataFrame(impute.fit_transform(pd.concat([df[cat_cols],df[cont_cols]],axis = 1)))\n    \n    df_temp.columns = cat_cols + cont_cols\n    \n    df_cat = df_temp[cat_cols].copy()\n    df_cont = df_temp[cont_cols].copy()\n    \n    # Decreasing data skewness for continuos variables using BoxCox\n    df_cont['golden_ratio'] = (df_cont['song_duration_ms'] / 1000) * 0.618033\n    df_cont['song_duration_ms'] = boxcox(df_cont.song_duration_ms/60000)[0]\n    \n    df_cont['instrumentalness_main'] = np.where(df_cont['instrumentalness']<=0.01,df_cont['instrumentalness'],0)\n    df_cont['instrumentalness_side'] = np.where(df_cont['instrumentalness']>0.2,df_cont['instrumentalness'],0)\n    df_cont.drop('instrumentalness',axis =1 ,inplace=True)\n    df_cont['instrumentalness_side'] = boxcox((df_cont.instrumentalness_side)+0.005)[0]\n    #df_cont['instrumentalness_main'] = boxcox((df_cont.instrumentalness_main)+0.2)[0]\n    \n    \n    df_cont['acousticness'] = boxcox(df_cont.acousticness + 0.2)[0]\n    \n    df_cont['liveness_main'] = np.where(df_cont['liveness']<0.8,df_cont['liveness'],0)\n    df_cont['liveness_side'] = np.where(df_cont['liveness']>=0.8,df_cont['liveness'],0)\n    df_cont.drop('liveness',axis =1 ,inplace=True)\n    #df_cont['liveness']  = boxcox(df_cont.liveness + 0.04)[0]    \n    \n    df_cont['loudness'] = boxcox(df_cont.loudness + 100)[0] \n    df_cont['energy'] = boxcox(df_cont.energy+0.2)[0]\n    df_cont['danceability'] = boxcox(df_cont.danceability)[0]\n  \n    df_cont['speechiness'] = boxcox(df_cont.speechiness)[0]\n    df_cont['tempo'] = boxcox(df_cont.tempo)[0]\n    \n    # Normalise Continuous columns\n    transformer = data_scaler_fit(option,df_cont)\n    df_cont = pd.DataFrame(transformer.transform(df_cont.apply(lambda x: remove_outliers(x,method))))\n    \n    df_cont.columns = [x for x in cont_cols if (x not in ['instrumentalness','liveness'])] + ['golden_ratio','instrumentalness_main','instrumentalness_side','liveness_main','liveness_side']\n    \n    df = pd.concat([df_cont,df_cat.reset_index(drop=True)],axis = 1)\n    \n    df['id'] = ids\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.893872Z","iopub.execute_input":"2022-01-29T11:41:07.894314Z","iopub.status.idle":"2022-01-29T11:41:07.909218Z","shell.execute_reply.started":"2022-01-29T11:41:07.894277Z","shell.execute_reply":"2022-01-29T11:41:07.908418Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Initialize Static values\nrandom_state = 151\nearly_stopping_rounds=1500\nverbose = 2000\nn_estimators = 15000\nn_splits = 5\nmethod = 'mean'\nscaling_option = 2\nimpute_method = 'lgbm'","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:07.910418Z","iopub.execute_input":"2022-01-29T11:41:07.910888Z","iopub.status.idle":"2022-01-29T11:41:07.920536Z","shell.execute_reply.started":"2022-01-29T11:41:07.910851Z","shell.execute_reply":"2022-01-29T11:41:07.919775Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### 4. Using Optuna for Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"df_train,df_test,df_submission = read_data()\n\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\ndef objective(trial):\n    # XGBoost parameters\n    \n    gc.collect()\n    \n    final_valid_predictions = {}\n    \n    scores = []\n    auc_score = []\n    \n    params = {\n        \"objective\": \"reg:squaredlogerror\",\n        \"n_estimators\": n_estimators,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),\n        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.9),\n        \"subsample\": trial.suggest_loguniform(\"subsample\", 0.2, 0.9),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 100.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"gamma\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 100),\n        \"n_jobs\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"predictor\" : \"gpu_predictor\",\n    }\n    \n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n        xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n        xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n\n        # Store IDs of validation Dataset\n        valid_ids = xvalid.id.values.tolist()\n\n        #Save a copy of yvalid\n        true_valid = yvalid\n\n        n_class = len(np.unique(ytrain))\n\n        xtrain = feature_transform(xtrain,scaling_option,method)\n        xvalid = feature_transform(xvalid,scaling_option,method)\n        \n        xtrain = xtrain.drop('id',axis = 1)\n        xvalid = xvalid.drop('id',axis = 1)\n        \n        \n        model = XGBRegressor(\n            random_state = random_state,\n            sampling_method = 'gradient_based',\n            use_label_encoder=False,\n            eval_metric = ['aucpr','auc'],\n            **params\n        )\n        model.fit(xtrain, ytrain,early_stopping_rounds=early_stopping_rounds, eval_set=[(xvalid, yvalid)], verbose=verbose)\n\n        preds_valid = model.predict(xvalid)\n        \n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n        auc_scr = roc_auc_score(true_valid, preds_valid)\n\n        auc_score.append(auc_scr)\n\n        print(f\"Fold {fold+1} || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    return roc_auc_score(y.to_numpy(), np.array(sorted(final_valid_predictions.items()))[:,1])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:41:11.294187Z","iopub.execute_input":"2022-01-29T11:41:11.294470Z","iopub.status.idle":"2022-01-29T11:41:11.432012Z","shell.execute_reply.started":"2022-01-29T11:41:11.294422Z","shell.execute_reply":"2022-01-29T11:41:11.431303Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"![alt text](https://miro.medium.com/max/900/1*80wf6AeqTLD9ntyFxYMuLw.jpeg)","metadata":{}},{"cell_type":"markdown","source":"### Keep Calm .. Optuna is Studying....","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T11:46:09.800059Z","iopub.execute_input":"2022-01-29T11:46:09.800345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Print best parameters ..","metadata":{}},{"cell_type":"code","source":"hp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:29:58.415636Z","iopub.execute_input":"2022-01-22T22:29:58.416189Z","iopub.status.idle":"2022-01-22T22:29:58.424004Z","shell.execute_reply.started":"2022-01-22T22:29:58.416147Z","shell.execute_reply":"2022-01-22T22:29:58.423078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Training XGB Model based on best Parameters","metadata":{}},{"cell_type":"code","source":"# Read DATA Again\ndf_train,df_test,df_submission = read_data()\n\n# Seperate X and y\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\n# Transform Test Dataset\ndf_test = feature_transform(df_test,scaling_option,method)\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\n\nscores = []\nauc_score = []\n\ncv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n    xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n    \n    xtest = df_test.copy().drop('id',axis = 1)\n\n    # Store IDs of validation Dataset\n    valid_ids = xvalid.id.values.tolist()\n    \n    #Save a copy of yvalid\n    true_valid = yvalid\n    \n    n_class = len(np.unique(ytrain))\n    \n    xtrain = feature_transform(xtrain,scaling_option,method)\n    xvalid = feature_transform(xvalid,scaling_option,method)\n    \n    xtrain = xtrain.drop('id',axis = 1)\n    xvalid = xvalid.drop('id',axis = 1)\n    \n    \n    static = {\n        \"n_estimators\": n_estimators,\n        \"objective\": \"reg:squaredlogerror\",\n        \"random_state\": random_state,\n        \"n_jobs\": -1,\n        #\"tree_method\": \"gpu_hist\",\n        #\"predictor\" : \"gpu_predictor\",\n        \"eval_metric\" : ['aucpr','auc'],\n        #\"sampling_method\" : 'gradient_based',\n        \"use_label_encoder\" : False,\n    }\n    \n    params = dict(static)\n\n    params.update(study.best_params)\n    \n\n    model = XGBRegressor(\n        **params\n    )\n    model.fit(xtrain, ytrain,early_stopping_rounds=early_stopping_rounds, eval_set=[(xtrain, ytrain),(xvalid, yvalid)], verbose=verbose)\n\n    preds_valid = model.predict(xvalid)\n\n    test_preds = model.predict(xtest)\n\n    final_test_predictions.append(test_preds)\n\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n    auc_scr = roc_auc_score(true_valid, preds_valid)\n    \n    auc_score.append(auc_scr)\n    \n    print('_'*65)\n    \n    print(f\"Fold {fold+1}  || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    print('_'*65)\n    \n    print('\\n')\n    \n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot Feature Importance\nplt.figure(figsize=(20, 15),dpi=80)\nplot_importance(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T21:20:54.055013Z","iopub.execute_input":"2022-01-28T21:20:54.056140Z","iopub.status.idle":"2022-01-28T21:20:54.374060Z","shell.execute_reply.started":"2022-01-28T21:20:54.056098Z","shell.execute_reply":"2022-01-28T21:20:54.372942Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['aucpr'])\nx_axis = range(0, epochs)\n# plot AUCPR\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['aucpr'], label='Train')\nax.plot(x_axis, results['validation_1']['aucpr'], label='Valid')\nax.legend()\nplt.ylabel('AUCPR Score')\nplt.title('XGBoost AUCPR')\nplt.show()\n# plot AUC\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Valid')\nax.legend()\nplt.ylabel('AUC Score')\nplt.title('XGBoost AUC')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:46:50.014923Z","iopub.execute_input":"2022-01-27T11:46:50.015195Z","iopub.status.idle":"2022-01-27T11:46:50.425834Z","shell.execute_reply.started":"2022-01-27T11:46:50.015166Z","shell.execute_reply":"2022-01-27T11:46:50.425137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Submit Score","metadata":{}},{"cell_type":"code","source":"df_submission.song_popularity = np.mean(np.column_stack(final_test_predictions), axis=1)\ndf_submission.columns = [\"id\", \"song_popularity\"]\ndf_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T21:21:11.281062Z","iopub.execute_input":"2022-01-28T21:21:11.281384Z","iopub.status.idle":"2022-01-28T21:21:11.323425Z","shell.execute_reply.started":"2022-01-28T21:21:11.281345Z","shell.execute_reply":"2022-01-28T21:21:11.322497Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"markdown","source":"***Do upvote if you found this useful*** \n\nPS: I am still working on improving this notebook and Model, so stay tuned!!!","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: center\" >\n<img src= \"https://media.giphy.com/media/DMHEccCwpNxCQBZlvQ/giphy.gif\"\n</p>","metadata":{}}]}